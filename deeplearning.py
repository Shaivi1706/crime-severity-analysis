# -*- coding: utf-8 -*-
"""DeepLearning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KxR2IBbUgxt46igXc8HLMznFONYqQkfk
"""

!pip install catboost xgboost
!pip install optuna-integration[catboost]
!pip install optuna

import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, f1_score
from sklearn.calibration import CalibratedClassifierCV
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from imblearn.combine import SMOTETomek
from catboost import CatBoostClassifier, Pool
from xgboost import XGBClassifier
import optuna
import matplotlib.pyplot as plt

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.utils import to_categorical

# Load the processed datasets
X_train = pd.read_csv('X_train (2).csv')
y_train = pd.read_csv('y_train (2).csv')
X_test = pd.read_csv('X_test (2).csv')
y_test = pd.read_csv('y_test (2).csv')
X_resampled = pd.read_csv('X_resampled (1).csv')
y_resampled = pd.read_csv('y_resampled (1).csv')

print("Data loaded successfully.")
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)
print("X_resampled shape:", X_resampled.shape)
print("y_resampled shape:", y_resampled.shape)

# Define categorical features based on X_train columns
cat_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()
print("Categorical features:", cat_features)

# ----------------- Compute class weights -----------------
from sklearn.utils.class_weight import compute_class_weight
# Ensure classes include all unique values in y_resampled and y_resampled is a 1D array/Series
classes = np.unique(y_resampled['severity_encoded'])
class_weights_arr = compute_class_weight('balanced', classes=classes, y=y_resampled['severity_encoded'].values)
class_weights = dict(zip(classes.astype(int), class_weights_arr)) # Convert keys to integers
display(class_weights)

print(y_resampled.isnull().values.any())
print(X_resampled.isnull().values.any())
print(y_test.isnull().values.any())
print(X_test.isnull().values.any())
print(y_train.isnull().values.any())
print(X_train.isnull().values.any())

y_train_cat = to_categorical(y_train)
y_test_cat = to_categorical(y_test)

"""#Sequential Baseline Model"""

# Apply one-hot encoding to categorical features
X_train_encoded = pd.get_dummies(X_train, columns=cat_features, drop_first=True)
X_test_encoded = pd.get_dummies(X_test, columns=cat_features, drop_first=True)

print("X_train_encoded shape:", X_train_encoded.shape)
print("X_test_encoded shape:", X_test_encoded.shape)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_encoded.shape[1],)),
    BatchNormalization(),
    Dropout(0.3),
    Dense(64, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    Dense(3, activation='softmax') # Output layer with 3 classes and softmax activation
])

model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

import tensorflow as tf
history = model.fit(
    X_train_encoded.values.astype(np.float32), y_train_cat,
    validation_data=(X_test_encoded.values.astype(np.float32), y_test_cat),
    epochs=50,
    batch_size=64,
    class_weight=class_weights,
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]
)

from sklearn.metrics import classification_report, confusion_matrix

y_pred_proba = model.predict(X_test_encoded.values.astype(np.float32))
y_pred = np.argmax(y_pred_proba, axis=1)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

"""#Summary

High class (0) is dominating with 0.88 F1, very
stable.

Low class (1) is weak — recall is 0.46 (meaning over half are missed).

Medium class (2) is okayish but still 0.55 recall & precision.

Confusion matrix shows heavy misclassification of low → medium and medium → high.

#Deep MLP
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.utils import to_categorical

y_train_one_hot = to_categorical(y_resampled['severity_encoded'], num_classes=len(classes))

# Apply one-hot encoding to X_resampled
X_resampled_encoded = pd.get_dummies(X_resampled, columns=cat_features, drop_first=True)

print("X_resampled_encoded shape:", X_resampled_encoded.shape)

# Deep MLP architecture
deep_mlp_model = Sequential([
    Dense(512, activation='relu', input_shape=(X_resampled_encoded.shape[1],)),
    BatchNormalization(),
    Dropout(0.4),

    Dense(256, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),

    Dense(128, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),

    Dense(64, activation='relu'),
    BatchNormalization(),
    Dropout(0.2),

    Dense(len(classes), activation='softmax')
])

deep_mlp_model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',  # Can replace with focal loss later
    metrics=['accuracy']
)

history = deep_mlp_model.fit(
    X_resampled_encoded.values.astype('float32'),
    y_train_one_hot,
    epochs=30,
    batch_size=64,
    validation_split=0.2,
    class_weight=class_weights,  # Using your computed weights
    verbose=1
)

# Predictions
y_pred_probs = deep_mlp_model.predict(X_test_encoded.values.astype('float32'))
y_pred_classes = np.argmax(y_pred_probs, axis=1)

# Report & Matrix
print("Classification Report:")
print(classification_report(y_test['severity_encoded'], y_pred_classes, digits=2))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test['severity_encoded'], y_pred_classes))

"""#Summary

Class 0 (high): Precision ↑ to 0.93, but recall dropped to 0.81 (Focusing on lows/mediums).

Class 1 (low): Recall up big (0.65), precision stable (0.52)

Class 2 (medium): Recall also increased to 0.68, with a small precision trade-off.

Macro avg recall ↑ from 0.63 → 0.71.

Accuracy stayed ~77–78%, but distribution is much less biased now.

#Residual MLP
"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Add, Activation
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np
import pandas as pd

# Assuming X_resampled_encoded is already available from previous steps
input_shape = X_resampled_encoded.shape[1]
num_classes = len(np.unique(y_resampled['severity_encoded']))

# Simple Residual Block
def simple_residual(x, units):
    shortcut = x
    x = Dense(units, activation='relu')(x)
    x = Dense(units)(x)
    x = Add()([shortcut, x])  # skip connection
    x = Activation('relu')(x)
    return x

# Build Model
inputs = Input(shape=(input_shape,))
x = Dense(128, activation='relu')(inputs)
x = simple_residual(x, 128)
x = Dropout(0.3)(x)
x = simple_residual(x, 128)
outputs = Dense(num_classes, activation='softmax')(x)

model = Model(inputs, outputs)

model.compile(optimizer=Adam(learning_rate=1e-3),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

model.fit(X_resampled_encoded.values.astype('float32'), y_resampled['severity_encoded'],
          validation_split=0.2,
          epochs=30,
          batch_size=64,
          class_weight=class_weights,
          callbacks=[es],
          verbose=1)

# Evaluate the Residual MLP model on the test data

# Make predictions on the encoded test data
y_pred_probs_residual = model.predict(X_test_encoded.values.astype('float32'))
y_pred_classes_residual = np.argmax(y_pred_probs_residual, axis=1)

# Print Classification Report
print("Residual MLP Classification Report:")
print(classification_report(y_test['severity_encoded'], y_pred_classes_residual, digits=2))

# Print Confusion Matrix
print("\nResidual MLP Confusion Matrix:")
print(confusion_matrix(y_test['severity_encoded'], y_pred_classes_residual))

"""#Summary

High class (0): 0.93 precision, 0.82 recall

Low class (1): Recall jumped to 0.74 . Precision also slightly improved.

Medium class (2): Pretty balanced now (0.54 precision, 0.68 recall).

#Residual MLP with Focal Loss
"""

import tensorflow as tf

def focal_loss(alpha=0.25, gamma=2.0):
    def focal_loss_fixed(y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        epsilon = tf.keras.backend.epsilon()
        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)

        cross_entropy = -y_true * tf.math.log(y_pred)
        weight = alpha * tf.math.pow(1 - y_pred, gamma)
        loss = weight * cross_entropy
        return tf.reduce_mean(tf.reduce_sum(loss, axis=1))
    return focal_loss_fixed

model.compile(
    optimizer='adam',
    loss=focal_loss(alpha=0.3, gamma=2.0),  # More aggressive toward minority
    metrics=['accuracy']
)

history = model.fit(
    X_resampled_encoded.values.astype('float32'),
    y_resampled['severity_encoded'],
    validation_data=(X_test_encoded.values.astype('float32'), y_test['severity_encoded']),
    class_weight=class_weights,
    epochs=30,
    batch_size=64
)

# Evaluate the Residual MLP with Focal Loss model on the test data

# Make predictions on the encoded test data
y_pred_probs_residual_focal = model.predict(X_test_encoded.values.astype('float32'))
y_pred_classes_residual_focal = np.argmax(y_pred_probs_residual_focal, axis=1)

# Print Classification Report
print("Residual MLP with Focal Loss Classification Report:")
print(classification_report(y_test['severity_encoded'], y_pred_classes_residual_focal, digits=2))

# Print Confusion Matrix
print("\nResidual MLP with Focal Loss Confusion Matrix:")
print(confusion_matrix(y_test['severity_encoded'], y_pred_classes_residual_focal))

"""#Failed!!

#Residual MLP + per-class α focal loss + threshold tuning setup
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers
from sklearn.metrics import classification_report, confusion_matrix

# ------------------ Residual MLP ------------------
def build_residual_mlp(input_dim, num_classes):
    inputs = layers.Input(shape=(input_dim,))

    # First dense layer
    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.3)(x)

    # Residual block 1
    shortcut = layers.Dense(256, kernel_regularizer=regularizers.l2(1e-4))(x)
    r = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)
    r = layers.BatchNormalization()(r)
    r = layers.Dropout(0.3)(r)
    r = layers.Dense(256, kernel_regularizer=regularizers.l2(1e-4))(r)
    r = layers.BatchNormalization()(r)
    x = layers.Add()([shortcut, r])
    x = layers.Activation('relu')(x)

    # Residual block 2
    shortcut = layers.Dense(128, kernel_regularizer=regularizers.l2(1e-4))(x)
    r = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)
    r = layers.BatchNormalization()(r)
    r = layers.Dropout(0.3)(r)
    r = layers.Dense(128, kernel_regularizer=regularizers.l2(1e-4))(r)
    r = layers.BatchNormalization()(r)
    x = layers.Add()([shortcut, r])
    x = layers.Activation('relu')(x)

    # Output
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    return models.Model(inputs, outputs)

# ------------------ Custom Per-Class α Focal Loss ------------------
def focal_loss(alpha, gamma=2.0):
    alpha = tf.constant(alpha, dtype=tf.float32)

    def loss_fn(y_true, y_pred):
        y_true = tf.cast(y_true, tf.int32)
        y_true_oh = tf.one_hot(y_true, depth=len(alpha))

        cross_entropy = -y_true_oh * tf.math.log(tf.clip_by_value(y_pred, 1e-8, 1.0))
        weights = alpha * tf.pow(1 - y_pred, gamma)
        fl = tf.reduce_sum(weights * cross_entropy, axis=1)
        return tf.reduce_mean(fl)

    return loss_fn

# ------------------ Training ------------------
num_classes = len(np.unique(y_resampled['severity_encoded']))
alpha_values = [0.3, 2.0, 1.0]  # [High, Low, Medium] -> Boost low class
gamma_value = 2.0

model = build_residual_mlp(X_resampled_encoded.shape[1], num_classes)
model.compile(optimizer='adam', loss=focal_loss(alpha_values, gamma=gamma_value), metrics=['accuracy'])

history = model.fit(
    X_resampled_encoded.values.astype('float32'),
    y_resampled['severity_encoded'],
    validation_data=(X_test_encoded.values.astype('float32'), y_test['severity_encoded']),
    epochs=50,
    batch_size=128,
    verbose=1
)

# ------------------ Threshold Tuning ------------------
y_probs = model.predict(X_test_encoded.values.astype('float32'))
thresholds = [0.5, 0.4, 0.45]  # Adjust per class [High, Low, Medium]

y_pred = []
for prob in y_probs:
    adjusted_probs = prob.copy()
    for i, thr in enumerate(thresholds):
        if prob[i] < thr:
            adjusted_probs[i] = 0
    y_pred.append(np.argmax(adjusted_probs))
y_pred = np.array(y_pred)

print("Classification Report with Per-Class α Focal Loss + Threshold Tuning:")
print(classification_report(y_test['severity_encoded'], y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test['severity_encoded'], y_pred))

"""#Failed!!

Residual MLP turned out to be best among all!!
"""